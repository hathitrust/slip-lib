# -------------------------------------------------------------------
# Development version: run-20.conf: pt/search SLIP chunking
#
# PRODUCTION version run-21.conf
#
# *******************************************************************
# *** 
# ***     run-22.conf (duff) MUST BE IDENTICAL TO THIS CONFIG.   
# ***
# *******************************************************************

# ---------------------------------------------------------------------
#                            Extractor
#  ---------------------------------------------------------------------
#  Extracts files to disk based on document_data_uses_class
#  configuration (below)
document_data_extractor_class = Document::Extractor

# ---------------------------------------------------------------------
#  Extension
#  ---------------------------------------------------------------------
#  Extra child fields and ID field as a function of
#  document_data_tokenizer_granulatity and document_data_class_type
#  configuration (below).
document_extension_base_class = Document::Doc::Extension

# ---------------------------------------------------------------------
#                            Tokenizer
# ---------------------------------------------------------------------
# Document::Tokenizer::{File|Token}
#
# Tokenizer subclasses implement text chunking for a Solr document "ocr" field
#   Tokenizer::File [OCR file content or XML files' text() nodes]
#      granularity=N
#         o "ocr" field = 1..N file's content from zip for full-text experimental indexing.
#                         N=1 is special case for normal item-level page indexing.
#      granularity=0
#         o "ocr" field = ALL files concatenated for normal large-scale indexing
#   Tokenizer::Token
#      granularity = 1..N
#         o "ocr" field = one of X "balanced" chunks of N tokens
#           where X minimizes the difference between tokens per chunk and N
#      granularity=0
#         o "ocr" field = ALL content from files, concatenated
document_data_tokenizer_class = Document::Tokenizer::File
document_data_tokenizer_granulatity = 1

# ---------------------------------------------------------------------
#            Document::Doc HAS-A: ::Data, ::vSolrMetadataAPI
# ---------------------------------------------------------------------
# METS data USEs configuration controls which files are extracted from
# the zip.
#
document_data_uses_class = Document::Conf::uses_1

# Document::Doc::Data::{File,Token}, types: flat,nested. Typically
# Data::<subclass> should match Tokenizer::<subclass>
#
document_data_class = Document::Doc::Data::File
document_data_class_type = flat

# Algorithm packages found under Document::Algorithms that implement
# the and only the execute() method. Typically used to apply specific
# additional processint to text data. Applied in order listed.
#
document_data_algorithm_classes = DeHyphenate

# Document::Doc::vSolrMetadataAPI::<various>
#
document_metadata_class  = Document::Doc::vSolrMetadataAPI::Schema_PTS_2

# ---------------------------------------------------------------------
# Plugins
# ---------------------------------------------------------------------
# Typically used to create additional Solr document fields from member
# data available on the ::Data and ::vSolrMetadataAPI objects.
#
plugin_for_Document::Doc::Data::File = 

#
# Delete by query
#
default_Solr_delete_field   = vol_id

#
# Search
#
default_Solr_search_fields   = hid,vol_id,pgnum,seq,record_no

# highlighting parameters
solr_hl_snippets = 200
solr_hl_fragsize = 300

#
# Sizes
#
num_shards_list    = 1


# Talking to solr
#
# We now get the solr url from an environment variable, and use basic auth to talk to solr

solr_basic_auth_token = ENV[PTSEARCH_SOLR_BASIC_AUTH]
engine_for_shard_1 = ENV[PTSEARCH_SOLR]
prod_engine_for_shard_1 = ENV[PTSEARCH_SOLR]

#
# shard to index directory map
#
dir_for_shard_1    = /htsolr/ptsearch/shards/1/data/index

#
# host-to-shard(s) map
#
shards_of_host_solr-sdr-dev  = 1

#
# shard(s)-to-host map
#
host_of_shard_1    = solr-sdr-ptsearch

#
# producer hosts / shards
#
producers_per_shard = 1
# producers_per_host should be ceiling of (num_shards * producers_per_shard) / num_hosts
producers_per_host = 1
producer_hosts     = moxie-1|moxie-2
solr_hosts         = solr-sdr-ptsearch

hosting_site       = macc

tomcats_run_as_user = tomcatrhel5
tomcat_pattern     = /l/local/apache-tomcat-ptsearch\s+-D

#
# Error triage - large-scale
# 
# Solr could not parse doc
max_indx_errors = 1000;
# Could not create OCR for Solr doc
max_ocr__errors = 1000;
# Could not get metadata for Solr doc
max_meta_errors = 5000;
# Server unavailable
max_serv_errors = 100;
# N >= numhosts * numproducers/host * queue_slice_size
# Currently 5 * 5 * 5 = 125 
max_no_indexer_avail = 130;
# Serious stuff
max_crit_errors = 5;
