# -------------------------------------------------------------------
# Run-20 config: pt/search Solr backend 
#
# DEVELOPMENT: SLIP chunking model.
#
# (Edit run-21,22 for PRODUCTION also)
# -------------------------------------------------------------------

# ---------------------------------------------------------------------
#                            Extractor
#  ---------------------------------------------------------------------
#  Extracts files to disk based on document_data_uses_class
#  configuration (below)
document_data_extractor_class = Document::Extractor

# ---------------------------------------------------------------------
#  Extension
#  ---------------------------------------------------------------------
#  Extra child fields and ID field as a function of
#  document_data_tokenizer_granulatity and document_data_class_type
#  configuration (below).
document_extension_base_class = Document::Doc::Extension

# ---------------------------------------------------------------------
#                            Tokenizer
# ---------------------------------------------------------------------
# Document::Tokenizer::{File|Token}
#
# Tokenizer subclasses implement text chunking for a Solr document "ocr" field
#   Tokenizer::File [OCR file content or XML files' text() nodes]
#      granularity=N
#         o "ocr" field = 1..N file's content from zip for full-text experimental indexing.
#                         N=1 is special case for normal item-level page indexing.
#      granularity=0
#         o "ocr" field = ALL files concatenated for normal large-scale indexing
#   Tokenizer::Token
#      granularity = 1..N
#         o "ocr" field = one of X "balanced" chunks of N tokens
#           where X minimizes the difference between tokens per chunk and N
#      granularity=0
#         o "ocr" field = ALL content from files, concatenated
document_data_tokenizer_class = Document::Tokenizer::File
document_data_tokenizer_granulatity = 1

# ---------------------------------------------------------------------
#            Document::Doc HAS-A: ::Data, ::vSolrMetadataAPI
# ---------------------------------------------------------------------
# METS data USEs configuration controls which files are extracted from
# the zip.
#
document_data_uses_class = Document::Conf::uses_1

# Document::Doc::Data::{File,Token}, types: flat,nested. Typically
# Data::<subclass> should match Tokenizer::<subclass>
#
document_data_class = Document::Doc::Data::File
document_data_class_type = flat

# Algorithm packages found under Document::Algorithms that implement
# the and only the execute() method. Typically used to apply specific
# additional processint to text data. Applied in order listed.
#
document_data_algorithm_classes = DeHyphenate

# Document::Doc::vSolrMetadataAPI::<various>
#
document_metadata_class  = Document::Doc::vSolrMetadataAPI::Schema_PTS_2

# ---------------------------------------------------------------------
# Plugins
# ---------------------------------------------------------------------
# Typically used to create additional Solr document fields from member
# data available on the ::Data and ::vSolrMetadataAPI objects.
#
plugin_for_Document::Doc::Data::File = 

#
# Delete by query
#
default_Solr_delete_field   = vol_id

#
# Search
#
default_Solr_search_fields   = hid,vol_id,pgnum,seq,record_no

# highlighting parameters
solr_hl_snippets = 200
solr_hl_fragsize = 300

#
# Sizes
#
num_shards_list    = 1

#
# Server key to virtual server URI 
#
#mbooks_solr_engines     = http://solr-sdr-dev:8112/ptsearch
#engine_for_shard_1      = http://solr-sdr-dev:8112/ptsearch
#prod_engine_for_shard_1 = http://solr-sdr-ptsearch:8080/ptsearch

mbooks_solr_engines     = https://staging.ptsearch.kubernetes.hathitrust.org:8443/solr/ptsearch
engine_for_shard_1      = https://staging.ptsearch.kubernetes.hathitrust.org:8443/solr/ptsearch
prod_engine_for_shard_1 = https://staging.ptsearch.kubernetes.hathitrust.org:8443/solr/ptsearch
mbooks_staging_basic_auth_token = c29scjpXdTM5Q2E5eHZaNU5GQjNweTdVRA==



#
# shard to index directory map
#
dir_for_shard_1    = /htsolr/ptsearch-dev/shards/1/data/index

#
# host-to-shard(s) map
#
shards_of_host_solr-sdr-dev  = 1

#
# shard(s)-to-host map
#
host_of_shard_1    = solr-sdr-dev

#
# producer hosts / shards
#
producers_per_shard = 1
# producers_per_host should be ceiling of (num_shards * producers_per_shard) / num_hosts
producers_per_host = 1
producer_hosts     = grog|punch
solr_hosts         = alamo

hosting_site       = macc

tomcats_run_as_user = tomcatrhel5
tomcat_pattern     = /l/local/apache-tomcat-ptsearch\s+-D

#
# Error triage - large-scale
# 
# Solr could not parse doc
max_indx_errors = 1000;
# Could not create OCR for Solr doc
max_ocr__errors = 1000;
# Could not get metadata for Solr doc
max_meta_errors = 5000;
# Server unavailable
max_serv_errors = 100;
# N >= numhosts * numproducers/host * queue_slice_size
# Currently 5 * 5 * 5 = 125 
max_no_indexer_avail = 130;
# Serious stuff
max_crit_errors = 5;
